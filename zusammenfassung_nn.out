\BOOKMARK [1][-]{section.1}{Neuronale Netze - kurzer \334berblick}{}% 1
\BOOKMARK [1][-]{section.2}{Pattern Recognition}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Data Feature}{section.2}% 3
\BOOKMARK [3][-]{subsubsection.2.1.1}{Preprocessing:}{subsection.2.1}% 4
\BOOKMARK [3][-]{subsubsection.2.1.2}{Comparison}{subsection.2.1}% 5
\BOOKMARK [2][-]{subsection.2.2}{Parametric Methods}{section.2}% 6
\BOOKMARK [3][-]{subsubsection.2.2.1}{Bayes Decision Theory}{subsection.2.2}% 7
\BOOKMARK [3][-]{subsubsection.2.2.2}{Mixture Densities}{subsection.2.2}% 8
\BOOKMARK [2][-]{subsection.2.3}{Classifier Discrimination Function}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.4}{Curse of Dimensionality}{section.2}% 10
\BOOKMARK [3][-]{subsubsection.2.4.1}{Principle Component Analysis \(PCA\)}{subsection.2.4}% 11
\BOOKMARK [2][-]{subsection.2.5}{Non-Parametric Methods}{section.2}% 12
\BOOKMARK [3][-]{subsubsection.2.5.1}{Parzen Window}{subsection.2.5}% 13
\BOOKMARK [3][-]{subsubsection.2.5.2}{k-nearest Neighours}{subsection.2.5}% 14
\BOOKMARK [3][-]{subsubsection.2.5.3}{Clustering}{subsection.2.5}% 15
\BOOKMARK [2][-]{subsection.2.6}{Fisher-Linear Discriminant}{section.2}% 16
\BOOKMARK [2][-]{subsection.2.7}{Perceptron}{section.2}% 17
\BOOKMARK [1][-]{section.3}{Recurrent Neural Networks}{}% 18
\BOOKMARK [2][-]{subsection.3.1}{Sequence Learning}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.2}{Elman vs. Jordan Networks - Simple RNN}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.3}{Aufbau}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.4}{Training}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.5}{Vanishing / Exploding Gradient}{section.3}% 23
\BOOKMARK [1][-]{section.4}{Backpropagation}{}% 24
\BOOKMARK [2][-]{subsection.4.1}{Regularization}{section.4}% 25
\BOOKMARK [3][-]{subsubsection.4.1.1}{Weight Elimination}{subsection.4.1}% 26
\BOOKMARK [3][-]{subsubsection.4.1.2}{Optimal Brain Damage}{subsection.4.1}% 27
\BOOKMARK [3][-]{subsubsection.4.1.3}{Cascade Correlation}{subsection.4.1}% 28
\BOOKMARK [3][-]{subsubsection.4.1.4}{Meiosis Network}{subsection.4.1}% 29
\BOOKMARK [1][-]{section.5}{Speech}{}% 30
\BOOKMARK [2][-]{subsection.5.1}{Speech Recognition}{section.5}% 31
\BOOKMARK [2][-]{subsection.5.2}{Acoustic Model}{section.5}% 32
\BOOKMARK [3][-]{subsubsection.5.2.1}{Hidden Markov Model}{subsection.5.2}% 33
\BOOKMARK [3][-]{subsubsection.5.2.2}{Time Delay Neural Network \(TDNN\)}{subsection.5.2}% 34
\BOOKMARK [2][-]{subsection.5.3}{Word Model}{section.5}% 35
\BOOKMARK [3][-]{subsubsection.5.3.1}{Time Alignment}{subsection.5.3}% 36
\BOOKMARK [3][-]{subsubsection.5.3.2}{Multi-State-TDNN}{subsection.5.3}% 37
\BOOKMARK [3][-]{subsubsection.5.3.3}{NN-HMM Hybride}{subsection.5.3}% 38
\BOOKMARK [3][-]{subsubsection.5.3.4}{Viterbi-Algorithmus}{subsection.5.3}% 39
\BOOKMARK [3][-]{subsubsection.5.3.5}{Forward-Algorithmus}{subsection.5.3}% 40
\BOOKMARK [3][-]{subsubsection.5.3.6}{Backward-Algorithmus}{subsection.5.3}% 41
\BOOKMARK [1][-]{section.6}{Learning Vector Quantization}{}% 42
\BOOKMARK [2][-]{subsection.6.1}{Vector Quantization}{section.6}% 43
\BOOKMARK [3][-]{subsubsection.6.1.1}{Applications}{subsection.6.1}% 44
\BOOKMARK [3][-]{subsubsection.6.1.2}{Training}{subsection.6.1}% 45
\BOOKMARK [2][-]{subsection.6.2}{Learning Vector Quantization}{section.6}% 46
\BOOKMARK [3][-]{subsubsection.6.2.1}{LVQ1}{subsection.6.2}% 47
\BOOKMARK [3][-]{subsubsection.6.2.2}{LVQ2}{subsection.6.2}% 48
\BOOKMARK [3][-]{subsubsection.6.2.3}{LVQ2.1}{subsection.6.2}% 49
\BOOKMARK [3][-]{subsubsection.6.2.4}{LVQ3}{subsection.6.2}% 50
\BOOKMARK [3][-]{subsubsection.6.2.5}{OLVQ}{subsection.6.2}% 51
\BOOKMARK [1][-]{section.7}{Self-Organizing-Maps}{}% 52
\BOOKMARK [2][-]{subsection.7.1}{Principles Of Self-Oragnized-Learning}{section.7}% 53
\BOOKMARK [3][-]{subsubsection.7.1.1}{Principle 1: Self-Amplification}{subsection.7.1}% 54
\BOOKMARK [3][-]{subsubsection.7.1.2}{Principle 2: Competition}{subsection.7.1}% 55
\BOOKMARK [3][-]{subsubsection.7.1.3}{Principle 3: Cooperation}{subsection.7.1}% 56
\BOOKMARK [3][-]{subsubsection.7.1.4}{Principle 4: Structural Information}{subsection.7.1}% 57
\BOOKMARK [3][-]{subsubsection.7.1.5}{Hebbian Learning}{subsection.7.1}% 58
\BOOKMARK [2][-]{subsection.7.2}{Self-Organizing-Maps}{section.7}% 59
\BOOKMARK [1][-]{section.8}{Reinfocement Learning}{}% 60
\BOOKMARK [2][-]{subsection.8.1}{Bellman Equation}{section.8}% 61
\BOOKMARK [2][-]{subsection.8.2}{Q-Learning}{section.8}% 62
\BOOKMARK [2][-]{subsection.8.3}{Temporal Difference Learning}{section.8}% 63
\BOOKMARK [3][-]{subsubsection.8.3.1}{SARSA}{subsection.8.3}% 64
\BOOKMARK [2][-]{subsection.8.4}{Challenges}{section.8}% 65
\BOOKMARK [1][-]{section.9}{Deep Learning in Computer Vision}{}% 66
\BOOKMARK [1][-]{section.10}{Neural Network Applications in Machine Translation}{}% 67
\BOOKMARK [2][-]{subsection.10.1}{Conventional Statistical MT}{section.10}% 68
\BOOKMARK [2][-]{subsection.10.2}{Neural MT}{section.10}% 69
\BOOKMARK [1][-]{section.11}{Speaker Independence}{}% 70
\BOOKMARK [2][-]{subsection.11.1}{Frequency Invariance}{section.11}% 71
\BOOKMARK [2][-]{subsection.11.2}{Multi-Speaker Reference Model}{section.11}% 72
\BOOKMARK [2][-]{subsection.11.3}{Cross-Language DNNs}{section.11}% 73
\BOOKMARK [1][-]{section.12}{Hand Writing}{}% 74
\BOOKMARK [1][-]{section.13}{Natural Language Processing}{}% 75
\BOOKMARK [2][-]{subsection.13.1}{Language Model}{section.13}% 76
\BOOKMARK [3][-]{subsubsection.13.1.1}{Word Embeddings}{subsection.13.1}% 77
\BOOKMARK [3][-]{subsubsection.13.1.2}{Word2Vec}{subsection.13.1}% 78
\BOOKMARK [2][-]{subsection.13.2}{Sentence Modeling}{section.13}% 79
\BOOKMARK [3][-]{subsubsection.13.2.1}{Dynamic k-max Convolutional NN}{subsection.13.2}% 80
\BOOKMARK [1][-]{section.14}{Gradient Optimizations and 2nd order Methods}{}% 81
\BOOKMARK [2][-]{subsection.14.1}{Logistic Regression}{section.14}% 82
\BOOKMARK [3][-]{subsubsection.14.1.1}{Gradient Descent - General Approach}{subsection.14.1}% 83
\BOOKMARK [3][-]{subsubsection.14.1.2}{Batch Gradient Descent}{subsection.14.1}% 84
\BOOKMARK [3][-]{subsubsection.14.1.3}{Stochastic Gradient Descent}{subsection.14.1}% 85
\BOOKMARK [3][-]{subsubsection.14.1.4}{Mini-Batch Gradient Descent}{subsection.14.1}% 86
\BOOKMARK [2][-]{subsection.14.2}{Learning Rate Scheduling}{section.14}% 87
\BOOKMARK [3][-]{subsubsection.14.2.1}{Adagrad - ADAptive GRADient method}{subsection.14.2}% 88
\BOOKMARK [3][-]{subsubsection.14.2.2}{Adadelta}{subsection.14.2}% 89
\BOOKMARK [3][-]{subsubsection.14.2.3}{RMSprop}{subsection.14.2}% 90
\BOOKMARK [3][-]{subsubsection.14.2.4}{Adam - ADAptive Moment estimation}{subsection.14.2}% 91
\BOOKMARK [1][-]{section.15}{Error Functions}{}% 92
\BOOKMARK [2][-]{subsection.15.1}{Binary Cross Entropy or Negative Log Likelihood}{section.15}% 93
\BOOKMARK [1][-]{section.16}{Activation Functions}{}% 94
\BOOKMARK [2][-]{subsection.16.1}{Step Function}{section.16}% 95
\BOOKMARK [2][-]{subsection.16.2}{Linear Function}{section.16}% 96
\BOOKMARK [2][-]{subsection.16.3}{Logistic / Sigmoid Function}{section.16}% 97
\BOOKMARK [2][-]{subsection.16.4}{Hyperbolic Tangent function}{section.16}% 98
\BOOKMARK [2][-]{subsection.16.5}{Softmax Function}{section.16}% 99
\BOOKMARK [2][-]{subsection.16.6}{Rectified Linear Unit}{section.16}% 100
