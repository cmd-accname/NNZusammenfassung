\section{Reinfocement Learning}
\label{sect:reinforcement-learning}
\begin{itemize}
	\item agent can act
	\item actions change the agent's future state
	\item scalar rewards for success
	\item (sequential decision making
\end{itemize}
\textit{Select actions to maximize the future reward}.\\
Examples of Reinforcement Learning:
\begin{itemize}
	\item Fly stunt maneuver in a	 helicopter
	\item Defeat the world champion at backgammon	
	\item Manage an investment portfolio
	\item Control a power station
	\item Make a humanoid robot walk
	\item Play many different Atari games better than humans	
\end{itemize}
\begin{figure}[h]
\includegraphics[scale=0.4]{agent-and-environment}
\end{figure}
\textbf{Policy} $a = \pi(s)$: probabilty distribution of actions given a state\\
\textbf{Value function} $Q^{\pi}(s, a)$: expected total reward from state $s$ and action $a$ under policy $\pi$
\[
Q^{\pi}(s, a) = \mathbb{E}[r_{t + 1} + \gamma r_{t + 2} + \gamma^2 r_{t + 2} + \gamma^3 r_{t + 3} + ... | s, a]
\]
\textbf{Policy-based RL}: search directly for the optimal policy $\pi$ (achieving maximum future reward)\\
\textbf{Value-based RL}: estimate optimal value function $Q^*(s, a)$ (maximum value achievable under any policy)\\
To train these networks we use \textit{Monte Carlo methods} to learn from experience.
\textbf{Policy-based Reinforcement Learning}: parameterize the policy (for example with a neural network) and based on the cumulative rewards, update the policy. We need to get the gradient of the rewards with respect to the policy:
\begin{itemize}
	\item Policy Gradients algorithm
	\item Online: update after episode
	\item Offline: update while in episodes
\end{itemize}
\begin{figure}[h]
\includegraphics[scale=0.4]{policy-gradients-with-neural-networks}
\end{figure}
For the image shown we can use a RNN to determine what our current state is given the last state. RL for where should we look next, given our current states.
\begin{figure}[h]
\includegraphics[scale=0.4]{RL-rrn-rl}
\end{figure} \\
RL gives use an reward on which we can calculate the backward pass (reward represents the gradient).
\begin{figure}[h]
\includegraphics[scale=0.4]{policy-gradient-with-neural-networks}
\end{figure}\\[2cm]

\subsection{Bellman Equation}
\label{ssect:bellman-equation}
\[
Q^{\pi}(s, a) = \mathbb{E}[r_{t + 1} + \gamma r_{t + 2} + \gamma^2 r_{t + 2} + \gamma^3 r_{t + 3} + ... | s, a]
\]
Recursively:
\[
Q^{\pi}(s, a) = \mathbb{E}_{s^{'}}[r_{t + 1} + \gamma Q^{\pi}(s^{'}, a^{'})| s, a]
\]
Optimal value function:
\[
Q^{*}(s, a) = \mathbb{E}_{s^{'}}[r_{t + 1} + \gamma Q^{\pi}(s^{'}, a^{'})| s, a]
\]
Value iteration solve the Ballman Equation:
\[
Q_{i + 1}(s, a) = \mathbb{E}_{s^{'}}[r_{t + 1} + \gamma Q_i(s^{'}, a^{'})| s, a]
\]
\subsection{Q-Learning}
\label{ssect:q-learning}
Objective function by \textit{mean-squared error} in Q-values:
\[
L(w) = \mathbb{E}[(r + \gamma max_{a^{'}}Q(s^{'}, a^{'}, w^{'} - Q(s, a, w))^2]
\]
Q-learning gradient:
\[
\frac{\delta L(w)}{\delta w} = \mathbb{E}[(r + \gamma max_{a^{'}}Q(s^{'}, a^{'}, w^{'} - Q(s, a, w)) \frac{\delta Q(s, a, w)}{\delta w}]
\]

\subsection{Temporal Difference Learning}
\label{ssect:temporal-difference-learning}
General update rule:
\[
Q(s_t, a_t) += learning_rate \cdot (td_target - Q(s_t, a_t))
\]
\subsubsection{SARSA}
\label{sssect:sarsa}
\[
Q(s_t, a_t) += \alpha [r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
\]
Q-value for a state-action is updated by an error, adjusted by the learning rate alpha.\\
TD Target for SARSA:
\[
R_{t + 1} + discount_factor \cdot Q(s_{t + 1}, a_{t + 1})
\]
TD Target for Q-Learning:
\[
R_{t + 1} + discount_factor \cdot maxQ(s_{t + 1}, a_{t + 1})
\]

\subsection{Challenges}
\label{ssect:challenges}
\begin{itemize}
	\item Data is sequential
	\item policy changes rapidly with small changes to Q-values
	\item scale of gradients and reward us unknown at the onset
\end{itemize}
\newpage